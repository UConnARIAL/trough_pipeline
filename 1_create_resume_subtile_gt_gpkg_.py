#!/usr/bin/env python3
"""
1_create_resume_subtile_gt_gpkg.py

is the stage specific logic after refactoring common code to gt_gpkg_common.py,

This script does the bulk of the work by taking subtile filtered binary masks generated by the inference model,
it skelitenizes and does basic graph theoretic analysis and generates stasts for each subtile
It also packages the orginal, filtered, rastor masks, vectorised mask, graph nodes, edges, componnets, cutlines etc.

It can be executed at tile level (sorted ranked order)
Given a PGC mosaic it can check and resume from where it stopped.

There is a helper script in utils to check the status of completed tiles.

Overall objective:
Builds per-tile GeoPackages (components/edges/nodes + per-tile global_stats + XML)
and a master GeoPackage (global_stats rows for all tiles, global_poly polygons + XML per tile),
plus a whole-mosaic aggregation table global_stats_summary (1 row) with properly weighted averages.

Decisions:
  • Orientation: keep ONLY step-weighted mean on [0,180) (drop edge-weighted).
  • global_poly attributes: store averages instead of raw counts for nodes/edges:
      - avg_graph_nodes_per_component = num_graph_nodes / num_components
      - avg_graph_edges_per_component = num_graph_edges / num_components

Project: Permafrost Discovery Gateway: Mapping and Analysing Trough Capilary Networks
PI      : Chandi Witharana
Authors : Michael Pimenta, Amal Perera
"""
#!/usr/bin/env python3

# --- standard library ---
import os
import sys
import re
import time
import glob
import gc
import argparse
import logging
import multiprocessing as mp
from pathlib import Path

# -------------------- config load (tomllib / tomli) --------------------

# --- third-party: numeric / data ---
import numpy as np
import pandas as pd
import psutil

# --- third-party: geospatial / geometry ---
import geopandas as gpd
import shapely.geometry as sgeom
import rasterio
from rasterio.crs import CRS
from rasterio.vrt import WarpedVRT
from rasterio.warp import Resampling

# --- third-party: image / graph ---
from skimage.morphology import skeletonize
from skan.csr import pixel_graph
import networkx as nx

# --- logging noise reduction ---
logging.getLogger("rasterio").setLevel(logging.WARNING)
logging.getLogger("fiona").setLevel(logging.WARNING)

# ---- SciPy optional (fast path) ----
try:
    from scipy.sparse.csgraph import connected_components as csgraph_connected_components
    HAVE_SCIPY = True
except Exception:
    HAVE_SCIPY = False

# -------------------- your domain utils (kept local folder) --------------------
# NOTE: gt_gpkg_common stays alongside this script as requested.
from gt_gpkg_common import (
    StageTimer,
    _records_to_gdf,
    filter_done_tiffs,
    sanity_check_paths,
    sqlite_fast_writes,
    logp,
    axial_mean_deg,
    check_gpkg_crs_all_layers,
    build_tile_structured_path,
    cfg_get,load_toml,
    resolve_workers,
    get_id_version,get_target_epsg,
    finalize_workers,
)

# -------------------- pipeline libs --------------------
from lib import add_from_mask_rastors as AdMsk
from lib import update_in_exp_cover as UpExpCov
from lib import get_cutline_vector_for_rasotor as CutLine

# -------------------- config helpers --------------------

def apply_thread_env(cfg: dict) -> None:
    # Keep your HPC-friendly defaults but config-driven
    blas = str(cfg_get(cfg, "runtime", "openblas_threads", default=1))
    omp  = str(cfg_get(cfg, "runtime", "omp_threads", default=1))
    os.environ["OPENBLAS_NUM_THREADS"] = blas
    os.environ["OMP_NUM_THREADS"] = omp
    # optional ogr hints
    os.environ.setdefault("OGR_SQLITE_SYNCHRONOUS", "OFF")
    os.environ.setdefault("OGR_SQLITE_CACHE", "200000")

# -------------------- small geometry helpers --------------------
def fast_pix_to_xy_affine(trf, rr, cc):
    rr = np.asarray(rr, dtype=float)
    cc = np.asarray(cc, dtype=float)
    x = trf.c + (cc + 0.5) * trf.a + (rr + 0.5) * trf.b
    y = trf.f + (cc + 0.5) * trf.d + (rr + 0.5) * trf.e
    return x, y

# -------------------- gpkg layer replace (robust reruns) --------------------
def gpkg_write_layer_replace_old(gdf: gpd.GeoDataFrame, gpkg_path: str | Path, layer: str) -> None:
    gpkg_path = str(gpkg_path)
    # Preferred: geopandas with if_exists (pyogrio)
    try:
        gdf.to_file(gpkg_path, layer=layer, driver="GPKG", if_exists="replace")
        return
    except TypeError:
        pass

    # Fallback: delete existing layer then write
    try:
        from osgeo import ogr
        if Path(gpkg_path).exists():
            ds = ogr.Open(gpkg_path, update=1)
            if ds is not None and ds.GetLayerByName(layer) is not None:
                ds.DeleteLayer(layer)
            ds = None
    except Exception as e:
        logging.debug(f"Could not delete layer '{layer}' in {gpkg_path}: {e}")

    gdf.to_file(gpkg_path, layer=layer, driver="GPKG")

def gpkg_write_layer_replace(gdf: gpd.GeoDataFrame, gpkg_path: str, layer: str) -> None:
    """
    Replace a layer in an existing/new GeoPackage without using if_exists/IF_EXISTS,
    because some Fiona/GDAL stacks reject IF_EXISTS for GPKG.
    """
    gpkg_path = str(gpkg_path)

    # Ensure parent exists
    Path(gpkg_path).parent.mkdir(parents=True, exist_ok=True)

    # Delete layer if it exists (OGR)
    try:
        from osgeo import ogr
        if Path(gpkg_path).exists():
            ds = ogr.Open(gpkg_path, update=1)
            if ds is not None:
                if ds.GetLayerByName(layer) is not None:
                    ds.DeleteLayer(layer)
                ds = None
    except Exception as e:
        logging.debug(f"OGR delete-layer failed (continuing): {e}")

    # Now write normally (no if_exists)
    gdf.to_file(gpkg_path, layer=layer, driver="GPKG")


# -------------------- per-file GPKG writer (subtile output) --------------------
def write_per_file_gpkg(
    cfg: dict,
    tile_out_dir: str,
    tif_path: str,
    crs_wkt: str,
    nodes_records_f: list[dict],
    edges_records_f: list[dict],
    components_records_f: list[dict],
    *,
    verbose: bool = False,
    tile_id: str | None = None,
):
    os.makedirs(tile_out_dir, exist_ok=True)

    stem = Path(tif_path).stem
    stem = re.sub(r'[_-]mask$', '', stem, flags=re.I)
    base = f"{stem}_TCN.gpkg"
    out_gpkg = str(Path(tile_out_dir) / base)

    # always rebuild fresh for this per-file gpkg
    if os.path.exists(out_gpkg):
        for suf in ("", "-wal", "-shm"):
            p = out_gpkg + suf
            try:
                if os.path.exists(p):
                    os.remove(p)
            except OSError as e:
                logging.warning(f"Could not remove {p}: {e}")

    layers = cfg_get(cfg, "layers", default={}) or {}
    nodes_layer = layers.get("nodes", "GraphTheoreticNodes")
    edges_layer = layers.get("edges", "GraphTheoreticEdges")
    comps_layer = layers.get("components", "GraphTheoreticComponents")

    nodes_gdf = _records_to_gdf(nodes_records_f, crs_wkt)
    edges_gdf = _records_to_gdf(edges_records_f, crs_wkt)
    comps_gdf = _records_to_gdf(components_records_f, crs_wkt)

    if all(g is None or g.empty for g in (nodes_gdf, edges_gdf, comps_gdf)):
        return

    written = False
    for layer_name, gdf in ((nodes_layer, nodes_gdf), (edges_layer, edges_gdf), (comps_layer, comps_gdf)):
        if gdf is None or gdf.empty:
            continue
        gdf.to_file(out_gpkg, layer=layer_name, driver="GPKG", mode=("w" if not written else "a"))
        written = True

    if verbose:
        logging.debug(f"[{tile_id or ''}] created per-file GPKG: {out_gpkg}")

    # ---- add cutlines (config-driven) ----
    cut_cfg = cfg_get(cfg, "cutlines", default={}) or {}
    if cut_cfg.get("enabled", True):
        cut_layer = layers.get("cutlines","InputMosaicCutlinesVector")
        if cut_layer in {nodes_layer, edges_layer, comps_layer}:
            cut_layer = f"ext_{cut_layer}"
        cut_cfg = cfg_get(cfg, "cutlines", default={}) or {}
        cutline_vec_path = CutLine.find_cutline_vector_path(
            cutline_root=cut_cfg.get("root"),
            tif_path=tif_path,
            tile_id=tile_id,
            # If your CL module supports root/pattern, pass them here.
            # If not, keep CL internal logic or update CL to use cfg.
        )
        logging.debug(f"[{tile_id or ''}] cutline_vec_path: {cutline_vec_path}")
        logging.debug(f"[{tile_id or ''}] tif path : {tif_path}")
        logging.debug(f"[{tile_id or ''}] tile id : {tile_id}")

        if not cutline_vec_path:
            logging.warning(f"No cutline vector found for {tif_path}; skipping cutlines layer.")
        else:
            gdf = CutLine.clip_vector_to_raster_extent(tif_path, cutline_vec_path, fix_invalid=False)
            if gdf.empty:
                logging.info(f"Cutline clip produced 0 features; not writing '{cut_layer}'.")
            else:
                gpkg_write_layer_replace(gdf, out_gpkg, cut_layer)
                logging.info(f"[{tile_id or ''}] wrote cutlines layer '{cut_layer}' to {out_gpkg} ({len(gdf)} feat)")

    # ---- add masks + vectorized mask (config-driven) ----
    vec_cfg = cfg_get(cfg, "vectorize", default={}) or {}
    #orig_mask_path = original_mask_for_subtile(tif_path, cfg)
    orig_mask_path = mask_path = build_tile_structured_path(
    subtile_path=tif_path,
    root=cfg_get(cfg, "io", "model_mask_dir"),
    postfix="_mask.tif",
    strip_postfixes=("_mask.tif",), # remove mask postfix for the given tif path
    )

    AdMsk.add_masks_and_vector(
        output_gpkg=out_gpkg,
        orig_mask_tif=orig_mask_path,
        cleaned_mask_tif=tif_path,
        target_srs=vec_cfg.get("target_srs", "EPSG:3338"),
        assume_src_srs=vec_cfg.get("assume_src_srs", "EPSG:3413"),
        build_overviews=tuple(vec_cfg.get("build_overviews", [2, 4, 8, 16])),
        eight_connected=bool(vec_cfg.get("eight_connected", True)),
        min_area=float(vec_cfg.get("min_area", 0.5)),
        simplify_tol=float(vec_cfg.get("simplify_tol", 0.25)),
    )

    # ---- exposure cover update (config-driven) ----
    exp_cfg = cfg_get(cfg, "exposure", default={}) or {}
    if exp_cfg.get("enabled", True):
        UpExpCov.add_date_exposure_gaussian(
            gpkg_path=out_gpkg,
            cutline_layer=exp_cfg.get("cutline_layer", layers.get("cutlines", "InputMosaicCutlinesVector")),
            date_field=exp_cfg.get("date_field", "ACQDATE"),
            mean_month=int(exp_cfg.get("mean_month", 7)),
            mean_day=int(exp_cfg.get("mean_day", 15)),
            sigma_days=float(exp_cfg.get("sigma_days", 45)),
            nodes_layer=nodes_layer,
            edges_layer=edges_layer,
            comps_layer=comps_layer,
            edges_length_field=exp_cfg.get("edges_length_field", "length_m"),
            comps_length_field=exp_cfg.get("comps_length_field", "total_length_m"),
            target_crs=exp_cfg.get("target_crs", "EPSG:3338"),
        )
    if verbose:
        logging.debug(f"[{tile_id or ''}] finished per-file packaging: {out_gpkg}")
    return out_gpkg

# -------------------- worker: process one subtile tif --------------------
def process_one_subtile(task):
    cfg, tile_id, tif_path, output_tiles_dir, verbose = task

    target_epsg = int(cfg_get(cfg, "crs", "target_epsg", default=3338))
    TARGET_CRS = CRS.from_epsg(target_epsg)
    DEFAULT_PIXEL_M = float(cfg_get(cfg, "crs", "default_pixel_m", default=0.5))
    COMPS_HEARTBEAT = int(cfg_get(cfg, "runtime", "comps_heartbeat", default=20000))

    file_id = os.path.basename(tif_path)
    pfx = f"[{tile_id}|{file_id}]"

    edges_records_f = []
    nodes_records_f = []
    components_records_f = []

    crs_wkt = None
    running_edge_id = 0
    running_node_id = 0

    try:
        with StageTimer(verbose, pfx, "OPEN"):
            with rasterio.open(tif_path) as src:
                src_crs = src.crs
                if src_crs is None:
                    raise ValueError(f"No CRS found in raster: {tif_path}")

                if src_crs != TARGET_CRS:
                    with WarpedVRT(src, crs=TARGET_CRS, resampling=Resampling.nearest) as vrt:
                        img = vrt.read(1).astype(np.uint8)
                        trf = vrt.transform
                        crs_wkt = vrt.crs.to_wkt()
                else:
                    img = src.read(1).astype(np.uint8)
                    trf = src.transform
                    crs_wkt = src_crs.to_wkt()

                px = abs(trf.a)
                py = abs(trf.e)
                step_len_m = 0.5 * (px + py) if (px > 0 and py > 0 and abs(px - py) / max(px, py) < 1e-6) else DEFAULT_PIXEL_M

    except Exception as e:
        logging.error(f"{pfx} open error: {e}")
        return False

    with StageTimer(verbose, pfx, "THRESH"):
        binary = img > 0
        if not np.any(binary):
            logp(verbose, logging.DEBUG, f"{pfx} empty mask → skip")
            return True

    with StageTimer(verbose, pfx, "SKEL"):
        skel = skeletonize(binary)

    with StageTimer(verbose, pfx, "GRAPH"):
        graph_csr, coords_or_ids = pixel_graph(skel.astype(bool), connectivity=2)
        if isinstance(coords_or_ids, np.ndarray) and coords_or_ids.ndim == 2 and coords_or_ids.shape[1] == 2:
            coords = coords_or_ids.astype(int)
        else:
            rr, cc = np.unravel_index(coords_or_ids, skel.shape)
            coords = np.column_stack([rr.astype(int), cc.astype(int)])

        # deterministic ordering of nodes
        idx_rc = [(idx, (int(r), int(c))) for idx, (r, c) in enumerate(coords)]
        idx_rc.sort(key=lambda t: (t[1][0], t[1][1]))
        id_remap = {old_idx: new_idx for new_idx, (old_idx, _) in enumerate(idx_rc)}
        rc_by_new = {new_idx: rc for new_idx, (_, rc) in enumerate(idx_rc)}

        old_by_new = np.empty(len(id_remap), dtype=np.int64)
        for old_idx, new_idx in id_remap.items():
            old_by_new[new_idx] = old_idx

        G = nx.Graph()
        for new_idx, rc in rc_by_new.items():
            G.add_node(new_idx, rc=rc)

        rows, cols = graph_csr.nonzero()
        for i, j in zip(rows, cols):
            if i == j:
                continue
            ni, nj = id_remap[i], id_remap[j]
            if ni == nj:
                continue
            if ni < nj:
                G.add_edge(ni, nj)

    degs = dict(G.degree())

    with StageTimer(verbose, pfx, "KEY NODES"):
        key_nodes = [n for n, d in degs.items() if d != 2]
        comps_for_cycles = list(nx.connected_components(G))
        for comp_nodes in comps_for_cycles:
            if comp_nodes and all(degs[n] == 2 for n in comp_nodes):
                seed = min(comp_nodes)
                if seed not in key_nodes:
                    key_nodes.append(seed)
        key_nodes.sort(key=lambda n: (G.nodes[n]["rc"][0], G.nodes[n]["rc"][1]))

        node_id_map = {}
        for n in key_nodes:
            node_id_map[n] = running_node_id
            r, c = G.nodes[n]["rc"]
            x, y = fast_pix_to_xy_affine(trf, [r], [c])
            degree = degs[n]
            nodes_records_f.append({
                "tile_id": tile_id,
                "file_id": file_id,
                "component_id": -1,  # filled later
                "global_component_id": None,
                "node_id": running_node_id,
                "degree": int(degree),
                "is_endpoint": int(degree == 1),
                "is_junction": int(degree >= 2),
                "geometry": sgeom.Point(float(x[0]), float(y[0]))
            })
            running_node_id += 1

    visited_edges = set()
    branches = []
    with StageTimer(verbose, pfx, "TRACE"):
        for kn in key_nodes:
            neighs = sorted(G.neighbors(kn))
            for nbr in neighs:
                e0 = (min(kn, nbr), max(kn, nbr))
                if e0 in visited_edges:
                    continue
                rc_chain = []
                prev, curr = kn, nbr
                pr, pc = G.nodes[prev]["rc"]; cr, cc = G.nodes[curr]["rc"]
                rc_chain.append((pr, pc)); rc_chain.append((cr, cc))
                visited_edges.add(e0)

                while G.degree(curr) == 2:
                    n0, n1 = sorted(G.neighbors(curr))
                    nxt = n0 if n0 != prev else n1
                    e2 = (min(curr, nxt), max(curr, nxt))
                    if e2 in visited_edges:
                        break
                    visited_edges.add(e2)
                    prev, curr = curr, nxt
                    r, c = G.nodes[curr]["rc"]
                    rc_chain.append((r, c))

                nx_a = kn
                nx_b = curr if (curr in key_nodes) else None
                node_a = node_id_map.get(nx_a, -1)
                node_b = node_id_map.get(nx_b, -1)
                is_open_edge = int(node_b == -1)

                chain_pixels = len(rc_chain)
                length_m = float(chain_pixels * step_len_m)

                seg_orients = []
                for i in range(chain_pixels - 1):
                    (r0, c0), (r1, c1) = rc_chain[i], rc_chain[i + 1]
                    seg_orients.append((np.degrees(np.arctan2(r1 - r0, c1 - c0)) % 180.0))
                orient = axial_mean_deg(seg_orients)

                branches.append({
                    "rc_chain": rc_chain,
                    "node_a": node_a,
                    "node_b": node_b,
                    "is_open_edge": is_open_edge,
                    "length_m": length_m,
                    "orientation": float(orient)
                })

    # ---------- Component labeling ----------
    V = G.number_of_nodes()
    if HAVE_SCIPY:
        n_comps, labels_old = csgraph_connected_components(graph_csr, directed=False)
        comp_id_by_new = labels_old[old_by_new]
        num_comps = int(n_comps)
    else:
        comps = list(nx.connected_components(G))
        comps.sort(key=lambda s: min(s))
        comp_id_by_new = np.empty(V, dtype=np.int64)
        for cidx, comp_nodes in enumerate(comps):
            for n in comp_nodes:
                comp_id_by_new[n] = cidx
        num_comps = len(comps)

    with StageTimer(verbose, pfx, "COMPS"):
        rc_to_nx = {G.nodes[n]["rc"]: n for n in G.nodes()}
        comp_to_lines = {}

        deg_arr = np.fromiter((degs[i] for i in range(V)), dtype=np.int64)
        nodes_per_comp = np.bincount(comp_id_by_new, minlength=num_comps)
        end_per_comp   = np.bincount(comp_id_by_new[deg_arr == 1], minlength=num_comps)
        junc_per_comp  = np.bincount(comp_id_by_new[deg_arr >= 2], minlength=num_comps)
        sumdeg_per_comp = np.bincount(comp_id_by_new, weights=deg_arr, minlength=num_comps).astype(np.float64)
        with np.errstate(divide="ignore", invalid="ignore"):
            avgdeg_per_comp = np.divide(sumdeg_per_comp, nodes_per_comp,
                                        out=np.zeros_like(sumdeg_per_comp), where=nodes_per_comp > 0)
        comp_len_m = nodes_per_comp.astype(np.float64) * float(step_len_m)

        edge_comp_ids = []
        edge_lengths  = []

        for b in branches:
            nx_first = rc_to_nx[b["rc_chain"][0]]
            comp_id  = int(comp_id_by_new[nx_first])
            global_component_id = f"{tile_id}|{file_id}|{comp_id}"

            rr = np.fromiter((r for r, _ in b["rc_chain"]), dtype=float)
            cc = np.fromiter((c for _, c in b["rc_chain"]), dtype=float)
            xs, ys = fast_pix_to_xy_affine(trf, rr, cc)
            line = sgeom.LineString(list(zip(xs.tolist(), ys.tolist())))

            rec_edge = {
                "tile_id": tile_id,
                "file_id": file_id,
                "component_id": comp_id,
                "global_component_id": global_component_id,
                "edge_id": running_edge_id,
                "node_id_a": int(b["node_a"]),
                "node_id_b": int(b["node_b"]),
                "is_open_edge": int(b["is_open_edge"]),
                "length_m": float(b["length_m"]),
                "orientation_deg_axial": float(b["orientation"]),
                "geometry": line
            }
            edges_records_f.append(rec_edge)
            running_edge_id += 1

            edge_comp_ids.append(comp_id)
            edge_lengths.append(float(b["length_m"]))
            comp_to_lines.setdefault(comp_id, []).append(line)

        # rewrite nodes with component ids
        nodes_records_f2 = []
        for n, nid in sorted(node_id_map.items(), key=lambda kv: kv[1]):
            r, c = G.nodes[n]["rc"]
            x, y = fast_pix_to_xy_affine(trf, [r], [c])
            degree = degs[n]
            comp_id = int(comp_id_by_new[n])
            global_component_id = f"{tile_id}|{file_id}|{comp_id}"
            nodes_records_f2.append({
                "tile_id": tile_id,
                "file_id": file_id,
                "component_id": comp_id,
                "global_component_id": global_component_id,
                "node_id": nid,
                "degree": int(degree),
                "is_endpoint": int(degree == 1),
                "is_junction": int(degree >= 2),
                "geometry": sgeom.Point(float(x[0]), float(y[0]))
            })
        nodes_records_f = nodes_records_f2

        # p95-trimmed mean branch length per component (components layer only)
        _edge_df = pd.DataFrame({"comp": edge_comp_ids, "len": edge_lengths})
        if not _edge_df.empty:
            p95 = _edge_df.groupby("comp")["len"].quantile(0.95)
            _edge_df = _edge_df.join(p95, on="comp", rsuffix="_p95")
            _edge_df = _edge_df[_edge_df["len"] <= _edge_df["len_p95"]]
            mean_b95_by_comp = _edge_df.groupby("comp")["len"].mean()
        else:
            mean_b95_by_comp = pd.Series(dtype=float)

        isolates = (nodes_per_comp == 1) & (end_per_comp == 0) & (junc_per_comp == 0)

        for cidx in range(num_comps):
            if (cidx % COMPS_HEARTBEAT) == 0 and cidx:
                logp(verbose, logging.DEBUG, f"{pfx} COMPS progress {cidx}/{num_comps}")

            lines = comp_to_lines.get(cidx, [])
            if isolates[cidx] and not lines:
                continue
            if not lines:
                ml = None
            else:
                ml = sgeom.MultiLineString([lines[0]] if len(lines) == 1 else lines)

            components_records_f.append({
                "tile_id": tile_id,
                "file_id": file_id,
                "component_local": int(cidx),
                "component_id": int(cidx),
                "global_component_id": f"{tile_id}|{file_id}|{int(cidx)}",
                "total_length_m": float(comp_len_m[cidx]),
                "num_edges": int(len(lines)),
                "num_nodes": int(nodes_per_comp[cidx]),
                "num_endnodes": int(end_per_comp[cidx]),
                "num_junctions": int(junc_per_comp[cidx]),
                "mean_branch_len_95_m": float(mean_b95_by_comp.get(cidx, 0.0)),
                "avg_degree": float(avgdeg_per_comp[cidx]),
                "geometry": ml
            })

    # ---- write per-file gpkg ----
    tile_out_dir = os.path.join(output_tiles_dir, str(tile_id))
    try:
        out_gpkg = write_per_file_gpkg(
            cfg,
            tile_out_dir,
            tif_path,
            crs_wkt,
            nodes_records_f,
            edges_records_f,
            components_records_f,
            verbose=verbose,
            tile_id=tile_id,
        )
    except Exception as e:
        logging.error(f"{pfx} per-file GPKG write error: {e}")
        return False

    problems = check_gpkg_crs_all_layers(out_gpkg, "EPSG:3338")
    if problems:
        for p in problems:
            logging.error(p)
        raise RuntimeError(f"CRS validation failed for {out_gpkg} ({len(problems)} issues)")
    else:
        logging.info(f"CRS validation PASS: {out_gpkg}")

    # cleanup
    del img, binary, skel, graph_csr, coords_or_ids, coords, G
    gc.collect()
    logp(verbose, logging.DEBUG, f"{pfx} done")
    return True

# -------------------- main --------------------

def main():
    import argparse, os, glob, logging, sys, psutil

    p = argparse.ArgumentParser("Step 1: Subtile processing")
    p.add_argument("--config", required=True)
    p.add_argument("--workers", type=int, default=None, help="Override workers (0=auto). If omitted, uses TOML.")
    p.add_argument("--one-tile", type=str, default=None)
    p.add_argument("--limit-tifs", type=int, default=None)
    p.add_argument("--verbose", action="store_true")
    p.add_argument("--rank", type=int, default=1, help="1-based rank for round-robin partition")
    p.add_argument("--world-size", type=int, default=1, help="Total ranks for round-robin partition")
    p.add_argument("--tile-rank-st", type=int, default=None, help="1-based slice start (after partition)")
    p.add_argument("--tile-rank-end", type=int, default=None, help="1-based slice end (after partition, inclusive)")
    p.add_argument("--tifs-per-run", type=int, default=None, help="Cap total TIFs processed this run (overrides TOML)")

    args = p.parse_args()

    cfg = load_toml(args.config)
    apply_thread_env(cfg)

    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        datefmt="%H:%M:%S",
        force=True,
    )

    master_dir = cfg_get(cfg, "io", "master_dir")
    output_tiles_dir = cfg_get(cfg, "io", "output_tiles_dir")
    if not master_dir or not output_tiles_dir:
        raise ValueError("Config must include [io].master_dir and [io].output_tiles_dir")

    # Step defaults from TOML, CLI can override
    one_tile = args.one_tile if args.one_tile is not None else (cfg_get(cfg, "steps", "subtile", "select", "one_tile", default="") or None)
    limit_tifs = args.limit_tifs if args.limit_tifs is not None else int(cfg_get(cfg, "steps", "subtile", "select", "limit_tifs", default=0) or 0)
    tifs_per_run = int(cfg_get(cfg, "steps", "subtile", "tifs_per_run", default=0) or 0)

    # workers resolution
    n_workers_cfg = resolve_workers(cfg, "subtile", args.workers)
    n_workers = finalize_workers(n_workers_cfg, cfg=cfg)

    id_version = get_id_version(cfg)
    target_epsg = get_target_epsg(cfg)
    logging.info(f"Step=subtile | id_version={id_version} | target_epsg={target_epsg} | workers={n_workers}")

    tiles = sorted([d for d in os.listdir(master_dir) if os.path.isdir(os.path.join(master_dir, d))])
    if args.one_tile:
        tiles = [t for t in tiles if t == args.one_tile]

    # discover pending tifs
    limit = int(args.limit_tifs or 0)
    remaining = limit if limit > 0 else float("inf")

    INF = float("inf")

    # ---------- discover all tiles ----------
    tiles = [
        d for d in sorted(os.listdir(master_dir))
        if os.path.isdir(os.path.join(master_dir, d))
    ]

    # apply one_tile (CLI > TOML)
    if one_tile:
        tiles = [t for t in tiles if t == one_tile]

    # ---------- rank/world partition (round-robin) ----------
    rank = max(1, int(args.rank or 1))
    world = max(1, int(args.world_size or 1))
    rank0 = rank - 1
    tiles = [t for i, t in enumerate(tiles) if (i % world) == rank0]
    logging.info(f"[rank {rank}/{world}] assigned {len(tiles)} tiles")

    # ---------- optional manual slice override (after partition) ----------
    st = args.tile_rank_st
    en = args.tile_rank_end
    if st is not None or en is not None:
        n = len(tiles)
        st = 1 if st is None else int(st)
        en = n if en is None else int(en)
        st_i, en_i = max(0, st - 1), min(n, en)
        tiles = tiles[st_i:en_i] if st_i < en_i else []
        logging.info(f"[rank {rank}] slice override {st}-{en} → {len(tiles)} tiles")

    # ---------- cap total TIFs for this run ----------
    # precedence: CLI --tifs-per-run > TOML steps.subtile.tifs_per_run > default 35
    cap_run = (
        int(args.tifs_per_run)
        if args.tifs_per_run is not None
        else int(cfg_get(cfg, "steps", "subtile", "tifs_per_run", default=35) or 35)
    )
    cap_run = INF if cap_run <= 0 else cap_run

    # debug cap / safety cap: limit_tifs (CLI > TOML)
    cap_limit = INF if (limit_tifs is None or int(limit_tifs) <= 0) else int(limit_tifs)

    remaining = min(cap_run, cap_limit)
    logging.info(f"[capacity] cap_run={cap_run} cap_limit={cap_limit} => remaining={remaining}")

    # ---------- build tasks ----------
    tasks = []
    for tile_id in tiles:
        if remaining <= 0:
            break

        tif_paths = sorted(glob.glob(os.path.join(master_dir, tile_id, "*.tif")))
        if not tif_paths:
            continue

        tile_out_dir = os.path.join(output_tiles_dir, str(tile_id))
        tif_paths, _skipped = filter_done_tiffs(tif_paths, tile_out_dir, verify=True, verbose=args.verbose)
        if not tif_paths:
            continue

        take = len(tif_paths) if remaining == INF else min(int(remaining), len(tif_paths))
        for tif in tif_paths[:take]:
            tasks.append((cfg, tile_id, tif, output_tiles_dir, args.verbose))

        if remaining != INF:
            remaining -= take

    if not tasks:
        logging.info("No pending TIFFs after filtering.")
        return

    total_mem_gb = psutil.virtual_memory().total / 1e9
    n_workers = resolve_workers(cfg, "subtile", args.workers)
    if args.verbose:
        n_workers = 1
    logging.info(f"Tasks: {len(tasks)} | Workers: {n_workers} | RAM {total_mem_gb:.1f} GB")

    if args.verbose:
        n_workers = 1  # ordered logs
    logging.info(f"Tasks: {len(tasks)} | Workers: {n_workers} | RAM {total_mem_gb:.1f} GB")

    t0 = time.time()
    ok = 0
    if n_workers == 1:
        for t in tasks:
            ok += 1 if process_one_subtile(t) else 0
    else:
        with mp.Pool(processes=n_workers) as pool:
            for res in pool.imap_unordered(process_one_subtile, tasks, chunksize=1):
                ok += 1 if res else 0

    dt = time.time() - t0
    logging.info(f"Done: {ok}/{len(tasks)} succeeded in {dt/60:.1f} min ({dt/max(1,len(tasks)):.2f} s/task)")

if __name__ == "__main__":
    try:
        main()
    except Exception:
        logging.exception("Fatal error")
        sys.exit(1)

"""
USAGE Minimal 
python 1_create_resume_subtile_gt_gpkg_.py --config ./config/subtile_gpkg.toml --verbose
python 1_create_resume_subtile_gt_gpkg_.py --config ./config.toml --tifs-per-run 5 --verbose

"""