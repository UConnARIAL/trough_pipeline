#!/bin/bash
#SBATCH --job-name=distributed_inference
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1  # Request 1 GPU per node (adjust as needed)
#SBATCH --time=01:00:00    # Set the maximum runtime (HH:MM:SS)
#SBATCH --mem=16GB         # Request memory per node (adjust as needed)
#SBATCH --output=inference_%j.out
#SBATCH --error=inference_%j.err

# Load any necessary modules (e.g., Python, CUDA)
# module load python/3.10
# module load cuda/11.8

# Get the hostname of the current node
HOSTNAME=$(hostname -s)

# Get the IP address of the current node
MASTER_ADDR=$(hostname -i | awk '{print $1}')

# Set the master port
MASTER_PORT=12355

# Get the number of GPUs available on the node
NPROC_PER_NODE=$(nvidia-smi --list-gpus | wc -l)

# Set the number of nodes and node rank (assuming single node for this example)
NNODES=1
NODE_RANK=0

# Your inference script and its arguments
INFERENCE_SCRIPT="inference.py"
SCRIPT_ARGS="" # Add any other arguments your inference.py needs

# Construct the torch.distributed.launch command
LAUNCH_CMD="python -m torch.distributed.launch"
LAUNCH_ARGS="--nproc_per_node=$NPROC_PER_NODE"
LAUNCH_ARGS+=" --nnodes=$NNODES"
LAUNCH_ARGS+=" --node_rank=$NODE_RANK"
LAUNCH_ARGS+=" --master_addr=\"$MASTER_ADDR\""
LAUNCH_ARGS+=" --master_port=$MASTER_PORT"

FULL_CMD="$LAUNCH_CMD $LAUNCH_ARGS $INFERENCE_SCRIPT $SCRIPT_ARGS"

echo "Running distributed inference with command:"
echo "$FULL_CMD"

# Execute the inference script
eval "$FULL_CMD"

echo "Distributed inference finished."
